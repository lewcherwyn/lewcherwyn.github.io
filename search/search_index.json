{"config":{"lang":["en","de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mywebsite","text":"<p>{newline}</p>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#cv","title":"CV","text":"<p>I attained my B.eng degree in Opto-electronic Information Science and Engineering after studying at the Jinan University, China.  Now I am a master student in Abbe School of Photonics, University of Jena, Germany. :&gt;)  Currently I am conducting my master\u2019s thesis and internship at the Leibniz IPHT, supervised by Prof. Dr. Markus Schmidt and Dr. Bennet Fischer.  My work mainly focuses on special fiber optics and metasurfaces.  ...</p>"},{"location":"gellery/Deepspaceimaging/","title":"Deep Space Imaging","text":""},{"location":"gellery/Deepspaceimaging/#2022","title":"2022","text":"NGC4038 NGC7000 M78 IC434 IC434_LRGB M81 NGC2237 M33 M1"},{"location":"gellery/Deepspaceimaging/#2021","title":"2021","text":"C/2021 A1(Leonard) C/2021 A1(Leonard) <p>In october 2021, I finally retired my ancient Canon 500d, the new ASI183MM camera has better sensitivity and lower dark current which gave me the opportunity to image some darker target.</p> <p> </p> NGC6960 <p> </p> M27 <p> </p> M31 <p> </p> M81&amp;M82 <p> </p> M104 <p> </p> NGC2264"},{"location":"gellery/Deepspaceimaging/#2020","title":"2020","text":"<p>Throughout 2020, I was almost entirely occupied with building my observatory, there were hardly any imaging tasks at that period...</p> <p> </p> Orion_2020"},{"location":"gellery/Deepspaceimaging/#2019","title":"2019","text":"M42 M45&amp;NGC1499 Orion_2019"},{"location":"gellery/Planetaryimaging/","title":"Planetary Imaging","text":""},{"location":"gellery/Planetaryimaging/#jupiter","title":"Jupiter","text":"Jupiter_gif2 Jupiter_gif1 Jupiter"},{"location":"gellery/Planetaryimaging/#saturn","title":"Saturn","text":"Saturn"},{"location":"gellery/Planetaryimaging/#venus","title":"Venus","text":"Venus"},{"location":"gellery/Planetaryimaging/#lunar-surface","title":"Lunar surface","text":"Tycho(lunar crater)"},{"location":"project/Meteordetection/","title":"Rapid Meteor Detection","text":""},{"location":"project/Meteordetection/#rapid-meteor-detection-my-bachelor-thesis","title":"Rapid Meteor Detection (my bachelor thesis)","text":"<p>Meteor detection is a very interesting topic. After completing most of my observatory construction work,  I finally had the opportunity to build my own meteor camera system. The hardware for the monitoring device  can be simple, all you need just a camera with lens. After that, we need a software such as Ufocapture  or RMS to detect the video stream and record the suspected meteor video.</p> <p>However, most of these software are based on motion-detection algorithms, and some small flickering such as  airplanes or bugs will trigger false alarms and record them. Therefore, it is necessary to manually  review the candidate videos of the night.</p>"},{"location":"project/Meteordetection/#can-the-process-of-manually-review-be-replaced-by-ai","title":"Can the process of manually review be replaced by AI?","text":"<p>Certainly. The task of reviewing the videos is simple and mechanical, as the advanced AI algorithms available  today are capable of recognizing almost any object. However, real-time detection places significant demands on  computer performance and most of the setups used for recording such as mini PC or Raspberry Pi, do not have a  powerful GPU. Therefore, compressing the entire video's information and focusing on meteor becomes highly useful.</p>"},{"location":"project/Meteordetection/#a-universal-solver","title":"A universal solver","text":"Meteor in the sky <p>We know that different meteor recording cameras have varying fields of view, resolutions, and background objects.  If we want to create a universal solution, we must standardize each video. Building upon the discussion in the previous  chapter, we need to extract meteor information as comprehensively as possible and eliminate background information.</p>"},{"location":"project/Meteordetection/#actually-a-vedio-a-data-cube","title":"Actually a vedio = a data cube","text":"data_cube <p>Actually, a video is a data cube extends in the time dimension(as frames). Through the data cube(In matlab or numpy,  we treat it as a three-dimensional matrix), we can obtain the variation of each pixel over time, as shown in figure above. Moreover, only part of the frames contain meteor information as shown in the figure below.</p> <p> </p> Meteor show up from frame 30 to frame 60 in the video <p>These frames constitute the video along the time dimension, but only a few pixels experience significant changes in their  grayscale values, while the majority of pixels represent unchanged background with overlaid varying noise. Consequently, In a meteor capture scene, pixels can be roughly categorized into four types: Background ground pixels,  Background sky pixels, meteor path pixels, moon or bright objects pixels, as illustrated in figure below. Below is a brief  analysis of the variation in these four representative types of pixels:</p> <p> </p> Four types of pixels <p>(a) Background ground pixels: Due to minimal illumination, they contain the least information and show minimal variation  along the time dimension, with grayscale values close to pure black.</p> <p>(b) Background sky pixels: They receive partial photon information influenced by moonlight or skylight, resulting in noticeably  higher brightness compared to the ground. However, they still contain very little information, and these pixels are greatly  affected by noise, leading to irregular changes in grayscale values at low numeric levels along the time dimension.</p> <p>(c) Meteor path pixels: These pixels belong to the sky background but show variation in grayscale values along the time  dimension when meteors or other luminous objects pass through them. This variation is characterized by a sudden increase in  grayscale values followed by a return to the level of the sky background.</p> <p>(d) Moon or bright objects pixels: These pixels receive the maximum and constant illumination, causing their grayscale  values to appear almost pure white along the time dimension.</p>"},{"location":"project/Meteordetection/#single-pixel-subtraction","title":"Single pixel subtraction","text":"<p>Based on the characteristics of the grayscale value changes in the four regions mentioned above, we can perform the  following operation on all pixels in the temporal dimension: $$New_{gray(i,j)}= Max_{gray(i,j)} - Avg_{gray(i,j)}$$ As a result, we obtain an image composed of the grayscale values of each pixel after subtraction, as shown in figure below.  In this way, the glowing and moving objects in the original video will be recorded in a single image, while non-glowing or  stationary objects will be completely subtracted, and more exciting is the background noise will be greatly reduced due to the  averaging process of grayscale values.</p> <p> </p> Comparison <p>It can be seen that irrelevant areas such as the background ground, background sky, and moon or other bright objects are almost  completely subtracted, while all glowing moving object information is well preserved. From this point, we have completed the  compression of the video and the standardization of the images, as the characteristics of the meteor are already quite clear  from these glowing trail information. </p>"},{"location":"project/Meteordetection/#ai-model-for-trail-detection","title":"AI model for trail detection","text":"<p>If we want a more elegantly automated review, we can introduce an object detection AI model for automatic classification.  This AI model should accurately identify meteor trails, and we can execute scripts to delete or retain videos based on the  recognition results. In my bachelor thesis, I used YOLOv3 and v4 models for training and modeling to recognize meteor and aircraft trail,  with aircraft trail being the primary objects causing false alarms.</p> <p> </p> Trail of meteor and aircraft <p>It's worth noting that since we only need to detect a single image, we don't require a powerful GPU. And the task can be accomplished  solely with the CPU. The entire model is written in Python, so it can even run on a Raspberry Pi. In the subsequent work, I also  attempted to develop a UI demo to provide a more intuitive user experience.</p> <p> </p> a UI demo"},{"location":"project/Multicore-fibeWFs/","title":"Multicore-Fiber Wavefront Sensor","text":""},{"location":"project/Multicore-fibeWFs/#multicore-fiber-wavefront-sensor-ongoing","title":"Multicore-fiber wavefront sensor (ongoing)","text":""},{"location":"project/Mytinyobservatory/","title":"My Tiny Observatory","text":""},{"location":"project/Mytinyobservatory/#my-tiny-observatory","title":"My Tiny Observatory","text":"Preview <p>My hometown has extremely low light pollution compared to the city(bortle scale is 2) and approximately 100 clear nights  are available for observation each year. If the observatory can be controlled remotely, the efficiency will be greatly improved.  This is a plan I've been consistently working on since my bachelor's study. It's a multidisciplinary project, primarily focused  on programming and mechanics... I'm happy to share the challenges I've encountered and the solutions I've developed.</p>"},{"location":"project/Mytinyobservatory/#the-central-system","title":"The central system","text":""},{"location":"project/Mytinyobservatory/#main-functions-of-the-central-system","title":"Main functions of the central system","text":"<p>The central system serves as the brain of the entire observatory. It not only needs to handle data collection (temperature, humidity, wind speed, etc.), real-time monitoring  image display but also control switches for numerous electrical appliances. Finally, it should be easy to  access and possess a certain level of upgradability.</p>"},{"location":"project/Mytinyobservatory/#how-to-achieve-cross-platform-access-on-any-device","title":"How to achieve cross-platform access on any device?","text":"<p>While most observations and imaging tasks require the use of PC, day-to-day monitoring is primarily done on mobile devices.  Developing an app solely for the ios or android is not a wise choice, the time cost will be greatly increased.  Is there a platform can cross-device smoothly access? Yes, that is web.</p>"},{"location":"project/Mytinyobservatory/#where-to-deploy-our-web-services","title":"Where to deploy our web services?","text":"<p>Since the device needs to run stably for a long time and has the lowest possible power consumption, a Raspberry Pi 4B run with  linux kernel is our best choice.</p> <p> </p> Basic ideal of the system <p> In the end, I decided the use of a web network deployed on a Raspberry Pi as the central control system for the observatory,  building the server backend using Python's Flask. Thanks to the Raspberry Pi's powerful expandability, I could utilize its built-in  pins to control relays and, consequently, control electrical appliances. Additionally, I integrated Arduino to control numerous  sensors for data collection and cameras to capture images in inside and outside. The most exciting part is that the cost of all the  materials is remarkably cheap compared to smart home device available on the market!</p> <p> </p> Web_UI"},{"location":"project/Twistedmetasurfaces/","title":"Twisted Metasurfaces","text":""},{"location":"project/Twistedmetasurfaces/#vector-beam-generation-via-twisted-metasurfaces-my-master-project","title":"Vector beam generation via twisted metasurfaces (my master project)","text":"Polarization ellipse change through the metaatom."},{"location":"project/Twistedmetasurfaces/#experiment-setup-v10","title":"Experiment setup v1.0","text":"Current experiment setup."},{"location":"project/Twistedmetasurfaces/#results","title":"Results","text":""},{"location":"project/Twistedmetasurfaces/#sem-images","title":"SEM images","text":"<p>The generated vector beam will be verified using a rotating polarizer at the output.</p> Radial beam generation Azimuthal beam generation"}]}